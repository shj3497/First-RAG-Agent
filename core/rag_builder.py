# core/rag_builder.py
import os
import uuid
import hashlib
from typing import List, Dict, Any
import pickle

# --- ìƒˆë¡œìš´ ì„í¬íŠ¸ ---
import chromadb
import tiktoken
import openai
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt  # Mecab ëŒ€ì‹  Oktë¥¼ ì„í¬íŠ¸

# --- ìƒˆë¡œìš´ ì„í¬íŠ¸ ---
# í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € ìŠ¤í¬ë˜í•‘ ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
# get_page_html ëŒ€ì‹  scrape_dynamic_contentë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
from core.scraper import get_all_page_urls, scrape_dynamic_content

# --- ìƒìˆ˜ ì •ì˜ ---
# ì´ íŒŒì¼(rag_builder.py)ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì–´ë–¤ í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë”ë¼ë„ í•­ìƒ í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì •í™•íˆ ê°€ë¦¬í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# os.path.realpath(__file__): í˜„ì¬ íŒŒì¼ì˜ ì‹¤ì œ ì ˆëŒ€ ê²½ë¡œ (.../core/rag_builder.py)
# os.path.dirname(...): ë””ë ‰í† ë¦¬ ê²½ë¡œë§Œ ì¶”ì¶œ (.../core)
# os.path.dirname(...): í•œ ë‹¨ê³„ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™ (...) -> ìµœì¢…ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT_PATH = os.path.dirname(
    os.path.dirname(os.path.realpath(__file__)))
VECTOR_DB_PATH = os.path.join(PROJECT_ROOT_PATH, "vector_db")

# VECTOR_DB_PATH ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤. (ìµœì´ˆ ì‹¤í–‰ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ)
os.makedirs(VECTOR_DB_PATH, exist_ok=True)

VECTOR_DB_COLLECTION_NAME = "mcp_rag_collection"
# BM25 ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œë¥¼ ìƒìˆ˜ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
BM25_INDEX_PATH = os.path.join(VECTOR_DB_PATH, "bm25_index.pkl")

# OpenAI ì„ë² ë”© ëª¨ë¸ê³¼ Tiktoken ì¸ì½”ë” ì´ë¦„ì„ ìƒìˆ˜ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
EMBEDDING_MODEL = "text-embedding-3-small"
ENCODING_NAME = "cl100k_base"
# í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆŒ ì²­í¬ì˜ í¬ê¸°ì™€ ì²­í¬ ê°„ì˜ ê²¹ì¹˜ëŠ” í† í° ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ë‘ëŠ” ì´ìœ ëŠ” ë¬¸ë§¥ì´ ì˜ë¦¬ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50

# --- í´ë¼ì´ì–¸íŠ¸ ë° ì¸ì½”ë” ì´ˆê¸°í™” ---
client = chromadb.PersistentClient(path=VECTOR_DB_PATH)
collection = client.get_or_create_collection(name=VECTOR_DB_COLLECTION_NAME)
# Tiktoken ì¸ì½”ë”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
encoding = tiktoken.get_encoding(ENCODING_NAME)
# OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
# ì„í¬íŠ¸ ì‹œì ì— í™˜ê²½ë³€ìˆ˜ê°€ ì—†ë”ë¼ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡, ì‹¤ì œ ì‚¬ìš© ì‹œ ìƒì„±í•©ë‹ˆë‹¤.
_openai_client: AsyncOpenAI | None = None


def _get_openai_client() -> AsyncOpenAI:
    """
    OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§€ì—° ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    í™˜ê²½ë³€ìˆ˜ `OPENAI_API_KEY`ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _openai_client
    if _openai_client is None:
        # api_keyë¥¼ ëª…ì‹œ ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë©´ SDKê°€ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì½ìŠµë‹ˆë‹¤.
        _openai_client = AsyncOpenAI()
    return _openai_client


# build_rag_from_path í•¨ìˆ˜ë¥¼ asyncë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
# Playwrightê°€ ë¹„ë™ê¸°(async) ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
async def build_rag_from_path(site_url: str) -> Dict[str, Any]:
    """
    ì§€ì •ëœ URLì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ì—¬ RAG ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ì—…ë°ì´íŠ¸í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ Suspense ë“± ë™ì  ì½˜í…ì¸ ë¥¼ í¬í•¨í•œ ìµœì¢… HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        site_url (str): sitemap.xmlì´ ìœ„ì¹˜í•œ ì‚¬ì´íŠ¸ì˜ URL (ì˜ˆ: https://www.megazone.com).

    Returns:
        Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬.
    """

    # 1. ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ëª¨ë“  í˜ì´ì§€ì˜ URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ì´ í•¨ìˆ˜ê°€ ê¸°ì¡´ì˜ `_find_html_files`ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.
    all_page_urls = await get_all_page_urls(site_url)
    if not all_page_urls:
        return {"status": "error", "message": f"ì§€ì •ëœ URL '{site_url}'ì—ì„œ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # --- 1. ì‚­ì œëœ í˜ì´ì§€ ì²˜ë¦¬ ---
    # DBì— ì €ì¥ëœ ëª¨ë“  í˜ì´ì§€ì˜ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    db_pages = _get_all_pages_from_db()
    crawled_pages_set = set(all_page_urls)

    # DBì—ëŠ” ìˆì§€ë§Œ ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ì—ëŠ” ì—†ëŠ” í˜ì´ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    pages_to_delete = [url for url in db_pages if url not in crawled_pages_set]

    if pages_to_delete:
        print(
            f"ì‚­ì œëœ í˜ì´ì§€ {len(pages_to_delete)}ê°œë¥¼ DBì—ì„œ ì œê±°í•©ë‹ˆë‹¤: {pages_to_delete}")
        _delete_vectors_by_url(pages_to_delete)

    # --- 2. ì‹ ê·œ/ë³€ê²½ëœ í˜ì´ì§€ ì²˜ë¦¬ ---
    processed_files_count = 0
    skipped_files_count = 0
    for page_url in all_page_urls:
        # get_page_htmlê³¼ _extract_text_from_htmlì„ í•©ì¹œ ìƒˆ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        text_content = await scrape_dynamic_content(page_url)
        if not text_content.strip():
            print(f"Skipped (no content): {page_url}")
            continue

        content_hash = _generate_content_hash(text_content)

        # --- 2a. ì½˜í…ì¸  ë³€ê²½ ê°ì§€ ---
        # DBì—ì„œ í•´ë‹¹ URLì˜ ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        existing_item = collection.get(where={"page_url": page_url}, limit=1)

        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆê³ , ì½˜í…ì¸  í•´ì‹œê°€ ë™ì¼í•˜ë©´ ë³€ê²½ì´ ì—†ëŠ” ê²ƒì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
        if existing_item['metadatas'] and existing_item['metadatas'][0]['content_hash'] == content_hash:
            print(f"Skipped (no changes): {page_url}")
            skipped_files_count += 1
            continue

        print(f"Processing (new or updated): {page_url}")

        # --- 2b. ì‹ ê·œ/ë³€ê²½ëœ ì½˜í…ì¸  ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§) ---
        chunks = _split_text_into_chunks(text_content)
        metadatas = [{"page_url": page_url, "content_hash": content_hash,
                      "chunk_text": chunk} for chunk in chunks]
        embeddings = await _embed_chunks(chunks)
        ids = [f"{page_url}#{i}" for i in range(len(chunks))]

        # ê¸°ì¡´ì— ìˆë˜ í˜ì´ì§€ê°€ ì—…ë°ì´íŠ¸ëœ ê²½ìš°, ì´ì „ ì²­í¬ë“¤ì„ ë¨¼ì € ì‚­ì œí•˜ì—¬ ê°œìˆ˜ ë³€ê²½ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
        if existing_item['ids']:
            _delete_vectors_by_url([page_url])

        _upsert_vectors_to_db(
            ids=ids, embeddings=embeddings, metadatas=metadatas)

        processed_files_count += 1

    # --- 3. ëª¨ë“  DB ì‘ì—… ì™„ë£Œ í›„ BM25 ì¸ë±ìŠ¤ ì¬ìƒì„± ---
    _build_and_save_bm25_index()

    return {
        "status": "success",
        "message": f"RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ. ì²˜ë¦¬: {processed_files_count}ê°œ, ë³€ê²½ ì—†ìŒ: {skipped_files_count}ê°œ, ì‚­ì œ: {len(pages_to_delete)}ê°œ.",
    }


def _build_and_save_bm25_index():
    """
    ChromaDBì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    RAG ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ì˜ ë§ˆì§€ë§‰ì— í˜¸ì¶œë˜ì–´ í•­ìƒ ìµœì‹  ìƒíƒœë¥¼ ìœ ì§€í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """

    print("ğŸ”„ BM25 ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
    # .get()ì˜ include íŒŒë¼ë¯¸í„°ì—ì„œ 'ids'ë¥¼ ì œê±°í•©ë‹ˆë‹¤. idsëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
    all_items = collection.get(include=['metadatas'])
    if not all_items or not all_items['ids']:
        print("âš ï¸ DBì— ë°ì´í„°ê°€ ì—†ì–´ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # --- ì•ˆì •ì„± ê°•í™”ë¥¼ ìœ„í•œ í•„í„°ë§ ë¡œì§ ì¶”ê°€ ---
    # BM25 ì¸ë±ì‹±ì„ ìœ„í•´ IDì™€ ì²­í¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ë˜, ë‚´ìš©ì´ ì—†ëŠ” ì²­í¬ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    valid_ids = []
    valid_chunks = []
    for i, metadata in enumerate(all_items['metadatas']):
        chunk_text = metadata.get('chunk_text', '').strip()
        if chunk_text:  # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
            valid_ids.append(all_items['ids'][i])
            valid_chunks.append(chunk_text)

    if not valid_chunks:
        print("âš ï¸ ìœ íš¨í•œ ë‚´ìš©ì´ ìˆëŠ” ë¬¸ì„œê°€ ì—†ì–´ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # --- í† í°í™” ë°©ì‹ ë³€ê²½ ---
    # Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”í¼ìŠ¤ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.
    print("Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    okt = Okt()
    # morphs ëŒ€ì‹  nounsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ë§Œ ì¶”ì¶œ, ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
    tokenized_corpus = [okt.nouns(chunk) for chunk in valid_chunks]
    print("í† í°í™” ì™„ë£Œ.")

    # BM25 ì¸ë±ìŠ¤ ìƒì„±
    bm25 = BM25Okapi(tokenized_corpus)

    # ê²€ìƒ‰ ì‹œ ì›ë³¸ ì²­í¬ë¥¼ ì¡°íšŒí•˜ê¸° ìœ„í•´ ì¸ë±ìŠ¤ì™€ ì›ë³¸ ë°ì´í„°ë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    bm25_data = {
        "bm25_index": bm25,
        "corpus_ids": valid_ids,       # í•„í„°ë§ëœ ID ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
        "corpus_chunks": valid_chunks  # í•„í„°ë§ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
    }

    with open(BM25_INDEX_PATH, "wb") as f:
        pickle.dump(bm25_data, f)

    print(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ. {len(valid_chunks)}ê°œ ë¬¸ì„œ ì²˜ë¦¬. ({BM25_INDEX_PATH})")


def _generate_content_hash(content: str) -> str:
    """
    ì£¼ì–´ì§„ ë¬¸ìì—´ ë‚´ìš©ì˜ SHA256 í•´ì‹œ ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ í•´ì‹œëŠ” íŒŒì¼ ë‚´ìš©ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” 'ì§€ë¬¸' ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    # hashlib.sha256: ì•ˆì „í•œ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
    # .encode('utf-8'): í•´ì‹œ ê³„ì‚°ì„ ìœ„í•´ ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # .hexdigest(): ê³„ì‚°ëœ í•´ì‹œë¥¼ 16ì§„ìˆ˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def _extract_text_from_html(html_content: str) -> str:
    """
    BeautifulSoup4ë¥¼ ì‚¬ìš©í•˜ì—¬ HTML ì½˜í…ì¸ ì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸ë¥¼ ì œê±°í•˜ê³ 
    ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    [ì£¼ì˜] ì´ í•¨ìˆ˜ëŠ” ë™ì  íƒ­ ì½˜í…ì¸  ì²˜ë¦¬ë¥¼ ìœ„í•´ ë” ì´ìƒ ë©”ì¸ ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ëŒ€ì‹  scraper.pyì˜ scrape_dynamic_content í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    soup = BeautifulSoup(html_content, 'lxml')

    # script, style, nav, footer, header íƒœê·¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©ì„ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.
    # RAGì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë³¸ë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë¶€ë¶„ì„ ì œê±°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
    for element in soup(["script", "style", "nav", "footer", "header"]):
        element.decompose()

    # .get_text() ë©”ì„œë“œëŠ” íƒœê·¸ë“¤ì„ ì œì™¸í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    # separator=" ": í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ ì‚¬ì´ì— ê³µë°±ì„ ë„£ì–´ ë‹¨ì–´ê°€ ë¶™ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    # strip=True: ê° í…ìŠ¤íŠ¸ ë¼ì¸ì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    text = soup.get_text(separator=" ", strip=True)
    return text


def _split_text_into_chunks(text: str) -> List[str]:
    """
    Tiktoken ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •ëœ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    # í…ìŠ¤íŠ¸ë¥¼ í† í° IDì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    tokens = encoding.encode(text)

    chunks = []
    # ì „ì²´ í† í° ê¸¸ì´ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    # CHUNK_SIZE - CHUNK_OVERLAP ë§Œí¼ ê±´ë„ˆë›°ë©´ì„œ ê²¹ì¹˜ëŠ” ì²­í¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    for i in range(0, len(tokens), CHUNK_SIZE - CHUNK_OVERLAP):
        chunk_tokens = tokens[i:i + CHUNK_SIZE]
        # í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë””ì½”ë”©í•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        chunks.append(encoding.decode(chunk_tokens))

    return chunks


async def _embed_chunks(chunks: List[str]) -> List[List[float]]:
    """
    OpenAIì˜ ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    # OpenAI APIëŠ” ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ì„ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
    response = await _get_openai_client().embeddings.create(
        input=chunks,
        model=EMBEDDING_MODEL
    )
    # API ì‘ë‹µì—ì„œ ì‹¤ì œ ì„ë² ë”© ë²¡í„° ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    return [embedding.embedding for embedding in response.data]


def _upsert_vectors_to_db(ids: List[str], embeddings: List[List[float]], metadatas: List[Dict]):
    """
    ChromaDB ì»¬ë ‰ì…˜ì— ID, ì„ë² ë”© ë²¡í„°, ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥(upsert)í•©ë‹ˆë‹¤.
    'upsert'ëŠ” IDê°€ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸í•˜ê³ , ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ì‚½ì…í•˜ëŠ” í¸ë¦¬í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
    """
    collection.upsert(
        ids=ids,
        embeddings=embeddings,
        metadatas=metadatas
    )

# TODO: ì•„ë˜ í•¨ìˆ˜ë“¤ì€ ì¶”í›„ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤.
# def _split_text_into_chunks(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
#     """Tiktoken ë˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ìˆëŠ” ì¡°ê°(ì²­í¬)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
#     pass

# def _embed_chunks(chunks: List[str]) -> List[List[float]]:
#     """OpenAIì˜ ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
#     pass

# def _upsert_vectors_to_db(vectors: List[List[float]], metadatas: List[Dict]):
#     """ChromaDBì— ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥(upsert)í•©ë‹ˆë‹¤."""
#     pass


def _get_all_pages_from_db() -> List[str]:
    """
    ChromaDBì—ì„œ ì €ì¥ëœ ëª¨ë“  ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™€,
    ì¤‘ë³µì„ ì œê±°í•œ í˜ì´ì§€ URL ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # .get()ì— ì•„ë¬´ ì¡°ê±´ ì—†ì´ í˜¸ì¶œí•˜ë©´ ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # include=['metadatas']ë¡œ ë©”íƒ€ë°ì´í„°ë§Œ íš¨ìœ¨ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    all_items = collection.get(include=['metadatas'])

    # ë©”íƒ€ë°ì´í„°ì—ì„œ 'page_url'ë§Œ ì¶”ì¶œí•˜ì—¬ setìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•œ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    if 'metadatas' in all_items and all_items['metadatas']:
        return list(set(item['page_url'] for item in all_items['metadatas']))
    return []


def _delete_vectors_by_url(urls: List[str]):
    """
    ì£¼ì–´ì§„ URL ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë²¡í„° ë°ì´í„°ë¥¼ DBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    # .delete()ì˜ where í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë©”íƒ€ë°ì´í„° ê°’ì„ ê°€ì§„ ì•„ì´í…œì„ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    collection.delete(where={"page_url": {"$in": urls}})
