# -*- coding: utf-8 -*-
from langgraph.graph import END, StateGraph
from typing import List, TypedDict

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

from core.tools.rag_search import RagSearchTool
from core.base_agent import get_chat_model
from core.history import get_history_store


# --- 0. ìƒìˆ˜ ì •ì˜ ---
# ëŒ€í™” ê¸°ë¡ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì˜ ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (ì§ˆë¬¸+ë‹µë³€)
# ì§ìˆ˜ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥ (ì§ˆë¬¸/ë‹µë³€ ìŒ)
MAX_CONVERSATION_HISTORY_MESSAGES = 20


# --- 1. Graph State ì •ì˜ ---
# ê·¸ë˜í”„ì˜ ê° ë…¸ë“œë¥¼ ê±°ì¹˜ë©´ì„œ ë°ì´í„°ê°€ ì €ì¥ë˜ê³  ì—…ë°ì´íŠ¸ë˜ëŠ” ìƒíƒœ ê°ì²´ì…ë‹ˆë‹¤.
# TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í•„ë“œì˜ íƒ€ì…ì„ ëª…í™•íˆ ì •ì˜í•©ë‹ˆë‹¤.
class GraphState(TypedDict):
    question: str  # ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
    documents: List[str]  # RAGì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
    generation: str  # LLMì´ ìƒì„±í•œ ë‹µë³€
    grade: str  # ë‹µë³€ í‰ê°€ ê²°ê³¼ (useful / not useful)
    iterations: int  # ì¬ì‹œë„ íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€ìš©)
    chat_history: List[dict]  # ì´ì „ ëŒ€í™” ê¸°ë¡
    is_new_topic: bool  # í˜„ì¬ ì§ˆë¬¸ì´ ìƒˆë¡œìš´ ì£¼ì œì¸ì§€ ì—¬ë¶€


# --- 2. ë…¸ë“œ(Node) í•¨ìˆ˜ ì •ì˜ ---
# ê° ë…¸ë“œëŠ” ê·¸ë˜í”„ì˜ í•œ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

async def classify_topic_node(state: GraphState):
    """
    ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì˜ ì£¼ì œì™€ ì´ì–´ì§€ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ¤” 0. ì£¼ì œ ë¶„ë¥˜ ---")
    question = state["question"]
    chat_history = state.get("chat_history", [])

    if not chat_history:
        print("ğŸ’¬ ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ì–´ ìƒˆë¡œìš´ ì£¼ì œë¡œ íŒë‹¨í•©ë‹ˆë‹¤.")
        return {"is_new_topic": True}

    # ëŒ€í™” ê¸°ë¡ì„ ê°„ë‹¨í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
    history_str = "\n".join(
        [f'{msg.get("role")}: {msg.get("content")}' for msg in chat_history]
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ëŒ€í™”ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. "
         "ì£¼ì–´ì§„ 'ì´ì „ ëŒ€í™”'ì™€ 'ìƒˆë¡œìš´ ì§ˆë¬¸'ì„ ë³´ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì˜ ì£¼ì œë¥¼ ì´ì–´ê°€ëŠ”ì§€ ì•„ë‹ˆë©´ ì™„ì „íˆ ìƒˆë¡œìš´ ì£¼ì œì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”. "
         "ë‹µë³€ì€ 'yes' (ì£¼ì œê°€ ì´ì–´ì§) ë˜ëŠ” 'no' (ìƒˆë¡œìš´ ì£¼ì œ) ë¡œë§Œ ê°„ê²°í•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤."),
        ("user",
         f"## ì´ì „ ëŒ€í™” (ìµœëŒ€ {MAX_CONVERSATION_HISTORY_MESSAGES // 2}ìŒ):\n{history_str}\n\n"
         f"## ìƒˆë¡œìš´ ì§ˆë¬¸:\n{question}\n\n"
         "ì´ ìƒˆë¡œìš´ ì§ˆë¬¸ì€ ì´ì „ ëŒ€í™”ì˜ ì£¼ì œì™€ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? (yes / no)")
    ])
    llm = get_chat_model(temperature=0)
    chain = prompt | llm
    result = await chain.ainvoke({})

    if "no" in result.content.lower():
        print("ğŸ’¬ LLMì´ ìƒˆë¡œìš´ ì£¼ì œë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")
        return {"is_new_topic": True}
    else:
        print("ğŸ’¬ LLMì´ ê¸°ì¡´ ì£¼ì œê°€ ì´ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")
        return {"is_new_topic": False}


async def retrieve_documents_node(state: GraphState):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ“„ 1. ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state["question"]
    iterations = state.get("iterations", 0)
    print(f"ğŸ” ê²€ìƒ‰ ì§ˆë¬¸: '{question}' (ì‹œë„ íšŸìˆ˜: {iterations + 1})")

    rag_tool = RagSearchTool()
    # execute í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì´ (str, int) íŠœí”Œì´ë¯€ë¡œ, ë‘ ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤.
    documents_string, doc_count = await rag_tool.execute(query=question)

    print(f"ğŸ“š ê²€ìƒ‰ëœ ìµœì¢… ë¬¸ì„œ ìˆ˜: {doc_count} ê°œ")
    # LangGraphì˜ ë‹¤ìŒ ë…¸ë“œë¡œ í¬ë§·íŒ…ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ì „ë‹¬í•©ë‹ˆë‹¤.
    return {"documents": [documents_string], "question": question, "iterations": iterations + 1}


async def generate_answer_node(state: GraphState):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- âœï¸ 2. ë‹µë³€ ìƒì„± ---")
    question = state["question"]
    documents = state["documents"]
    chat_history = state.get("chat_history", [])
    is_new_topic = state.get("is_new_topic", True)

    # í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤. "
         "ì œê³µëœ ë¬¸ì„œì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user",
         "## ë¬¸ì„œ:\n\n---\n\n{documents}\n\n---\n\n## ì§ˆë¬¸:\n{question}")
    ])

    # LLM ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    llm = get_chat_model()

    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì—°ê²°í•©ë‹ˆë‹¤(LCEL).
    chain = prompt | llm

    # ìƒˆë¡œìš´ ì£¼ì œì¸ ê²½ìš°, ëŒ€í™” ê¸°ë¡ì„ ë¹„ì›Œì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
    effective_history = chat_history if not is_new_topic else []

    # ëŒ€í™” ê¸°ë¡ì˜ í˜•ì‹ì„ LangChain ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    history_messages = []
    for msg in effective_history:
        if msg.get("role") == "user":
            history_messages.append(HumanMessage(content=msg.get("content")))
        elif msg.get("role") == "assistant":
            history_messages.append(AIMessage(content=msg.get("content")))

    # LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    generation = await chain.ainvoke({
        "documents": "\n\n".join(documents),
        "question": question,
        "chat_history": history_messages
    })
    print(f"ğŸ’¬ ìƒì„±ëœ ë‹µë³€: {generation.content[:100]}...")

    return {"generation": generation.content}


async def grade_answer_node(state: GraphState):
    """
    ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€, ë¬¸ì„œ ë‚´ìš©ì„ ì˜ ë°˜ì˜í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ¤” 3. ë‹µë³€ í‰ê°€ ---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    # í‰ê°€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    # 'ìœ ìš©ì„±'ì´ë¼ëŠ” ëª¨í˜¸í•œ ê¸°ì¤€ ëŒ€ì‹ , ë‹µë³€ì´ 'ì •ë‹µ'ì„ í¬í•¨í•˜ëŠ”ì§€ë¥¼ ëª…í™•íˆ ë¬»ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” AI ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤. "
         "ì£¼ì–´ì§„ 'ì‚¬ìš©ì ì§ˆë¬¸'ì— ëŒ€í•œ ì •ë‹µì´ 'ê²€ìƒ‰ëœ ë¬¸ì„œ'ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. "
         "ê·¸ í›„, 'ìƒì„±ëœ ë‹µë³€'ì´ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ ì •í™•í•˜ê²Œ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”. "
         "ë‹µë³€ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ ëª…í™•íˆ í¬í•¨í•˜ê³  ìˆë‹¤ë©´ 'yes', ê·¸ë ‡ì§€ ì•Šê±°ë‚˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ê³  ë‹µë³€í•˜ë©´ 'no'ë¼ê³ ë§Œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤."),
        ("user",
         f"## ê²€í†  ì •ë³´\n\n"
         f"### ê²€ìƒ‰ëœ ë¬¸ì„œ:\n{''.join(documents)}\n\n"
         f"### ì‚¬ìš©ì ì§ˆë¬¸:\n{question}\n\n"
         f"### ìƒì„±ëœ ë‹µë³€:\n{generation}\n\n"
         "## í‰ê°€\n\nìƒì„±ëœ ë‹µë³€ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ? (yes / no)")
    ])
    llm = get_chat_model()
    chain = prompt | llm

    grade_result = await chain.ainvoke({})
    grade = grade_result.content
    print(f"ğŸ“ í‰ê°€ ê²°ê³¼: {grade}")

    # í‰ê°€ ê¸°ì¤€ì„ 'yes'ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    if "yes" in grade.lower():
        return {"grade": "useful"}  # 'useful' ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ê¸°ì¡´ íë¦„ê³¼ ë§ì¶¥ë‹ˆë‹¤.
    else:
        return {"grade": "not useful"}


async def rewrite_question_node(state: GraphState):
    """
    ë‹µë³€ì˜ ì ìˆ˜ê°€ ë‚®ì„ ê²½ìš°, ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ”„ 4. ì§ˆë¬¸ ì¬ì‘ì„± ---")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    is_new_topic = state.get("is_new_topic", True)

    # ìƒˆë¡œìš´ ì£¼ì œì´ê±°ë‚˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´, í˜„ì¬ ì§ˆë¬¸ë§Œìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    if is_new_topic:
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ì–´(search query)ë¡œ ë³€í™˜í•´ì£¼ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤."
             "ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ, RAG ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”."
             "ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ê°„ê²°í•˜ê²Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."),
            ("user", f"## ì›ë˜ ì§ˆë¬¸:\n{question}\n\n"
                     "ì´ ì§ˆë¬¸ì„ RAG ê²€ìƒ‰ì— ë” ì í•©í•˜ë„ë¡ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.")
        ])
    # ê¸°ì¡´ ì£¼ì œê°€ ì´ì–´ì§€ëŠ” ê²½ìš°, ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.
    else:
        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤.
        history_str = "\n".join(
            [f'{msg.get("role")}: {msg.get("content")}' for msg in chat_history]
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ì–´(search query)ë¡œ ë³€í™˜í•´ì£¼ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤."
             "ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ, ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ RAG ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”."
             "ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ê°„ê²°í•˜ê²Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."),
            ("user", f"## ì´ì „ ëŒ€í™”:\n{history_str}\n\n"
                     f"## í˜„ì¬ ì§ˆë¬¸:\n{question}\n\n"
                     "ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ RAG ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ ì§ˆë¬¸ì„ í•˜ë‚˜ë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.")
        ])

    llm = get_chat_model()
    chain = prompt | llm

    rewritten_question = await chain.ainvoke({})
    print(f"âœ¨ ì¬ì‘ì„±ëœ ì§ˆë¬¸: {rewritten_question.content}")

    return {"question": rewritten_question.content}


# --- 3. ì¡°ê±´ë¶€ ì—£ì§€(Edge) ë¡œì§ ---
# íŠ¹ì • ë…¸ë“œë¥¼ ì‹¤í–‰í•œ í›„, ì–´ë–¤ ë…¸ë“œë¡œ ì´ë™í• ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

def should_continue(state: GraphState):
    """
    ë‹µë³€ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ì›Œí¬í”Œë¡œìš°ë¥¼ ê³„ì†í• ì§€, ì•„ë‹ˆë©´ ì¢…ë£Œí• ì§€ ê²°ì •í•©ë‹ˆë‹¤.
    ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
    """
    grade = state.get("grade")
    iterations = state.get("iterations", 0)

    if iterations > 3:
        print("--- âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ---")
        return "end"

    if grade == "useful":
        print("--- âœ… ë‹µë³€ì´ ìœ ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ---")
        return "end"
    else:
        print("--- âŒ ë‹µë³€ì´ ìœ ìš©í•˜ì§€ ì•Šì•„ ì§ˆë¬¸ ì¬ì‘ì„± ì‹œë„ ---")
        return "continue"


# --- 4. ê·¸ë˜í”„ ë¹Œë“œ ---
# langgraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

workflow = StateGraph(GraphState)

# ë…¸ë“œë“¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("classify_topic", classify_topic_node)
workflow.add_node("retrieve", retrieve_documents_node)
workflow.add_node("generate", generate_answer_node)
workflow.add_node("grade", grade_answer_node)
workflow.add_node("rewrite", rewrite_question_node)

# ì—£ì§€ë“¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€í•˜ì—¬ ë…¸ë“œ ê°„ì˜ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.
workflow.set_entry_point("classify_topic")  # ì‹œì‘ì  ë³€ê²½
workflow.add_edge("classify_topic", "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "grade")

# ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
# 'grade' ë…¸ë“œ ì‹¤í–‰ í›„ 'should_continue' í•¨ìˆ˜ì˜ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°ë©ë‹ˆë‹¤.
# - "continue"ë¥¼ ë°˜í™˜í•˜ë©´ "rewrite" ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.
# - "end"ë¥¼ ë°˜í™˜í•˜ë©´ ì›Œí¬í”Œë¡œìš°ê°€ ì¢…ë£Œ(END)ë©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "grade",
    should_continue,
    {
        "continue": "rewrite",
        "end": END
    }
)
workflow.add_edge("rewrite", "retrieve")  # ì¬ì‘ì„± í›„ ë‹¤ì‹œ ê²€ìƒ‰ ë‹¨ê³„ë¡œ ìˆœí™˜

# ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
app = workflow.compile()


# --- 5. ì‹¤í–‰ í•¨ìˆ˜ ---
async def run_langgraph_agent(user_query: str, session_id: str = None) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ langgraphë¡œ êµ¬ì„±ëœ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    session_idë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ë° ì´ì „ ê¸°ë¡ ë¡œë“œ
    history = get_history_store()
    past_messages = []
    if session_id:
        print(
            f"ğŸ§  ì„¸ì…˜ '{session_id}'ì—ì„œ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤ (ìµœëŒ€ {MAX_CONVERSATION_HISTORY_MESSAGES}ê°œ).")
        past_messages = history.get_messages(session_id)[
            -MAX_CONVERSATION_HISTORY_MESSAGES:]

    # 2. ê·¸ë˜í”„ ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ê°’ êµ¬ì„±
    # is_new_topicì€ ì²« ë…¸ë“œì—ì„œ ê²°ì •ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì´ˆê¸°í™”í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    inputs = {"question": user_query, "chat_history": past_messages}

    # 3. LangGraph ì—ì´ì „íŠ¸ ì‹¤í–‰
    final_state = await app.ainvoke(inputs)
    final_answer = final_state["generation"]

    # 4. ëŒ€í™” ê¸°ë¡ ì €ì¥
    if session_id:
        new_messages = [
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": final_answer}
        ]
        history.add_messages(session_id, new_messages)
        print(
            f"ğŸ’¾ ì„¸ì…˜ '{session_id}'ì— ëŒ€í™” ê¸°ë¡ {len(new_messages)}ê°œ ì €ì¥ ì™„ë£Œ")

    # 5. ìµœì¢… ë‹µë³€ ë°˜í™˜
    return final_answer


# ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
if __name__ == '__main__':
    # .env íŒŒì¼ ë¡œë“œ (OPENAI_API_KEY)
    import asyncio
    from dotenv import load_dotenv
    import os

    # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ .env íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
    load_dotenv(dotenv_path=dotenv_path)

    async def _test_run():
        query = "MCP í¬í„¸ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        print(f"ğŸš€ LangGraph ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§ˆë¬¸: '{query}')")
        final_answer = await run_langgraph_agent(query)
        print("\n\n---\nğŸ ìµœì¢… ë‹µë³€:\n---")
        print(final_answer)

    asyncio.run(_test_run())
