# -*- coding: utf-8 -*-
from langgraph.graph import END, StateGraph
from typing import List, TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from core.tools.rag_search import RagSearchTool
from core.base_agent import get_chat_model


# --- 1. Graph State ì •ì˜ ---
# ê·¸ë˜í”„ì˜ ê° ë…¸ë“œë¥¼ ê±°ì¹˜ë©´ì„œ ë°ì´í„°ê°€ ì €ì¥ë˜ê³  ì—…ë°ì´íŠ¸ë˜ëŠ” ìƒíƒœ ê°ì²´ì…ë‹ˆë‹¤.
# TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í•„ë“œì˜ íƒ€ì…ì„ ëª…í™•íˆ ì •ì˜í•©ë‹ˆë‹¤.
class GraphState(TypedDict):
    question: str  # ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
    documents: List[str]  # RAGì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
    generation: str  # LLMì´ ìƒì„±í•œ ë‹µë³€
    grade: str  # ë‹µë³€ í‰ê°€ ê²°ê³¼ (useful / not useful)
    iterations: int  # ì¬ì‹œë„ íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€ìš©)


# --- 2. ë…¸ë“œ(Node) í•¨ìˆ˜ ì •ì˜ ---
# ê° ë…¸ë“œëŠ” ê·¸ë˜í”„ì˜ í•œ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

async def retrieve_documents_node(state: GraphState):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ“„ 1. ë¬¸ì„œ ê²€ìƒ‰ ---")
    question = state["question"]
    iterations = state.get("iterations", 0)
    print(f"ğŸ” ê²€ìƒ‰ ì§ˆë¬¸: '{question}' (ì‹œë„ íšŸìˆ˜: {iterations + 1})")

    rag_tool = RagSearchTool()
    # execute í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì´ (str, int) íŠœí”Œì´ë¯€ë¡œ, ë‘ ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤.
    documents_string, doc_count = await rag_tool.execute(query=question)

    print(f"ğŸ“š ê²€ìƒ‰ëœ ìµœì¢… ë¬¸ì„œ ìˆ˜: {doc_count} ê°œ")
    # LangGraphì˜ ë‹¤ìŒ ë…¸ë“œë¡œ í¬ë§·íŒ…ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ì „ë‹¬í•©ë‹ˆë‹¤.
    return {"documents": [documents_string], "question": question, "iterations": iterations + 1}


async def generate_answer_node(state: GraphState):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- âœï¸ 2. ë‹µë³€ ìƒì„± ---")
    question = state["question"]
    documents = state["documents"]

    # í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤. "
         "ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
        ("user",
         f"## ë¬¸ì„œ:\n\n---\n\n{{documents}}\n\n---\n\n## ì§ˆë¬¸:\n{{question}}")
    ])

    # LLM ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    llm = get_chat_model()

    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì—°ê²°í•©ë‹ˆë‹¤(LCEL).
    chain = prompt | llm

    # LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    generation = await chain.ainvoke(
        {"documents": "\n\n".join(documents), "question": question})
    print(f"ğŸ’¬ ìƒì„±ëœ ë‹µë³€: {generation.content[:100]}...")

    return {"generation": generation.content}


async def grade_answer_node(state: GraphState):
    """
    ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€, ë¬¸ì„œ ë‚´ìš©ì„ ì˜ ë°˜ì˜í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ¤” 3. ë‹µë³€ í‰ê°€ ---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    # í‰ê°€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    # 'ìœ ìš©ì„±'ì´ë¼ëŠ” ëª¨í˜¸í•œ ê¸°ì¤€ ëŒ€ì‹ , ë‹µë³€ì´ 'ì •ë‹µ'ì„ í¬í•¨í•˜ëŠ”ì§€ë¥¼ ëª…í™•íˆ ë¬»ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” AI ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤. "
         "ì£¼ì–´ì§„ 'ì‚¬ìš©ì ì§ˆë¬¸'ì— ëŒ€í•œ ì •ë‹µì´ 'ê²€ìƒ‰ëœ ë¬¸ì„œ'ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. "
         "ê·¸ í›„, 'ìƒì„±ëœ ë‹µë³€'ì´ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ ì •í™•í•˜ê²Œ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”. "
         "ë‹µë³€ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ ëª…í™•íˆ í¬í•¨í•˜ê³  ìˆë‹¤ë©´ 'yes', ê·¸ë ‡ì§€ ì•Šê±°ë‚˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ê³  ë‹µë³€í•˜ë©´ 'no'ë¼ê³ ë§Œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤."),
        ("user",
         f"## ê²€í†  ì •ë³´\n\n"
         f"### ê²€ìƒ‰ëœ ë¬¸ì„œ:\n{''.join(documents)}\n\n"
         f"### ì‚¬ìš©ì ì§ˆë¬¸:\n{question}\n\n"
         f"### ìƒì„±ëœ ë‹µë³€:\n{generation}\n\n"
         "## í‰ê°€\n\nìƒì„±ëœ ë‹µë³€ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ? (yes / no)")
    ])
    llm = get_chat_model()
    chain = prompt | llm

    grade_result = await chain.ainvoke({})
    grade = grade_result.content
    print(f"ğŸ“ í‰ê°€ ê²°ê³¼: {grade}")

    # í‰ê°€ ê¸°ì¤€ì„ 'yes'ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    if "yes" in grade.lower():
        return {"grade": "useful"}  # 'useful' ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ê¸°ì¡´ íë¦„ê³¼ ë§ì¶¥ë‹ˆë‹¤.
    else:
        return {"grade": "not useful"}


async def rewrite_question_node(state: GraphState):
    """
    ë‹µë³€ì˜ ì ìˆ˜ê°€ ë‚®ì„ ê²½ìš°, ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.
    """
    print("--- ğŸ”„ 4. ì§ˆë¬¸ ì¬ì‘ì„± ---")
    question = state["question"]

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ì–´(search query)ë¡œ ë³€í™˜í•´ì£¼ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤."
         "ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ, RAG ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”."
         "ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ê°„ê²°í•˜ê²Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."),
        ("user", f"## ì›ë˜ ì§ˆë¬¸:\n{question}\n\n"
                 "ì´ ì§ˆë¬¸ì„ RAG ê²€ìƒ‰ì— ë” ì í•©í•˜ë„ë¡ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.")
    ])
    llm = get_chat_model()
    chain = prompt | llm

    rewritten_question = await chain.ainvoke({})
    print(f"âœ¨ ì¬ì‘ì„±ëœ ì§ˆë¬¸: {rewritten_question.content}")

    return {"question": rewritten_question.content}


# --- 3. ì¡°ê±´ë¶€ ì—£ì§€(Edge) ë¡œì§ ---
# íŠ¹ì • ë…¸ë“œë¥¼ ì‹¤í–‰í•œ í›„, ì–´ë–¤ ë…¸ë“œë¡œ ì´ë™í• ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

def should_continue(state: GraphState):
    """
    ë‹µë³€ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ì›Œí¬í”Œë¡œìš°ë¥¼ ê³„ì†í• ì§€, ì•„ë‹ˆë©´ ì¢…ë£Œí• ì§€ ê²°ì •í•©ë‹ˆë‹¤.
    ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
    """
    grade = state.get("grade")
    iterations = state.get("iterations", 0)

    if iterations > 3:
        print("--- âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ---")
        return "end"

    if grade == "useful":
        print("--- âœ… ë‹µë³€ì´ ìœ ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ---")
        return "end"
    else:
        print("--- âŒ ë‹µë³€ì´ ìœ ìš©í•˜ì§€ ì•Šì•„ ì§ˆë¬¸ ì¬ì‘ì„± ì‹œë„ ---")
        return "continue"


# --- 4. ê·¸ë˜í”„ ë¹Œë“œ ---
# langgraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

workflow = StateGraph(GraphState)

# ë…¸ë“œë“¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("retrieve", retrieve_documents_node)
workflow.add_node("generate", generate_answer_node)
workflow.add_node("grade", grade_answer_node)
workflow.add_node("rewrite", rewrite_question_node)

# ì—£ì§€ë“¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€í•˜ì—¬ ë…¸ë“œ ê°„ì˜ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.
workflow.set_entry_point("retrieve")  # ì‹œì‘ì  ì„¤ì •
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "grade")

# ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
# 'grade' ë…¸ë“œ ì‹¤í–‰ í›„ 'should_continue' í•¨ìˆ˜ì˜ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°ë©ë‹ˆë‹¤.
# - "continue"ë¥¼ ë°˜í™˜í•˜ë©´ "rewrite" ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.
# - "end"ë¥¼ ë°˜í™˜í•˜ë©´ ì›Œí¬í”Œë¡œìš°ê°€ ì¢…ë£Œ(END)ë©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "grade",
    should_continue,
    {
        "continue": "rewrite",
        "end": END
    }
)
workflow.add_edge("rewrite", "retrieve")  # ì¬ì‘ì„± í›„ ë‹¤ì‹œ ê²€ìƒ‰ ë‹¨ê³„ë¡œ ìˆœí™˜

# ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
app = workflow.compile()


# --- 5. ì‹¤í–‰ í•¨ìˆ˜ ---
async def run_langgraph_agent(user_query: str, session_id: str = None) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ langgraphë¡œ êµ¬ì„±ëœ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ì„¸ì…˜ IDëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, `run_agent` ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•©ë‹ˆë‹¤.)
    """
    inputs = {"question": user_query}
    final_state = await app.ainvoke(inputs)

    # ìµœì¢… ìƒíƒœì—ì„œ ìƒì„±ëœ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return final_state["generation"]


# ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
if __name__ == '__main__':
    # .env íŒŒì¼ ë¡œë“œ (OPENAI_API_KEY)
    import asyncio
    from dotenv import load_dotenv
    import os

    # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ .env íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
    load_dotenv(dotenv_path=dotenv_path)

    async def _test_run():
        query = "MCP í¬í„¸ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        print(f"ğŸš€ LangGraph ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§ˆë¬¸: '{query}')")
        final_answer = await run_langgraph_agent(query)
        print("\n\n---\nğŸ ìµœì¢… ë‹µë³€:\n---")
        print(final_answer)

    asyncio.run(_test_run())
